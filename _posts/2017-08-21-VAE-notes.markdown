---
layout: post
title:  "Notes on Variational Autoencoder"
date:   2017-08-20 15:10:00
categories: main
---

## First notes: http://kvfrans.com/variational-autoencoders-explained/
## Background:
* Plain GAN can only generate image from random noise
* Generated image cannot be conditioned on certain variable
* No constraint on how the generated image looks like

## VAE intro:
* Consider a deconvolutional network that able to generate an image which characteristics are determined by an input vector
* Changes on the distribution of this input vector corresponds to a visual features of the output image
* We refer to this input vector as a latent variable
* Now consider a standard autoencoder that starts with convolutions and ended with deconvolutions, the input vector is now the intermediate vector representation
* VAE add some constraint to the network to encode latent vector that roughly follows unit gaussian distribution
* We do so by adding up two loss terms: the generative loss (measure the quality of the constructed images), the latent loss (KL divergence measure of the latent vector to gaussian distribution)

## VAE how:
* Applies re-parameterization trick to the encoder network: implicitly model mean and standard deviation of the latent vector
* The architecture will be roughly: input -> convolutions -> {mean vector\|std vector} -> sampled latent vector -> deconvolutions -> output image
* latent vector = z_mean + (z_stdev * sampled_normal[0,1])
* as the training goes, it gets more efficient: when the std raised until 1, less information is lost (?)

[jekyll-gh]: https://github.com/mojombo/jekyll
[jekyll]:    http://jekyllrb.com